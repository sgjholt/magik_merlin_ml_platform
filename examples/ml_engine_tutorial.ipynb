{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engine Tutorial\n",
    "\n",
    "This notebook demonstrates the custom ML engine capabilities of the Magik Merlin ML Platform.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. ü§ñ **AutoML Pipeline** - Automated model comparison\n",
    "2. ‚öôÔ∏è **Hyperparameter Optimization** - Using Optuna for tuning\n",
    "3. üìä **Model Evaluation** - Performance metrics and comparison\n",
    "4. üîç **Feature Importance** - Understanding model decisions\n",
    "5. üéØ **Individual Models** - Using specific models directly\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "uv sync --extra ml\n",
    "# OR\n",
    "pip install xgboost lightgbm catboost optuna scikit-learn pandas numpy matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ML Engine imports\n",
    "from core.ml_engine import (\n",
    "    AutoMLPipeline,\n",
    "    XGBoostClassifier,\n",
    "    model_registry,\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Registry\n",
    "\n",
    "The ML engine includes a centralized model registry for discovering available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available models\n",
    "all_models = model_registry.list_models()\n",
    "print(\"üìã All available models:\")\n",
    "for model in all_models:\n",
    "    print(f\"   ‚Ä¢ {model}\")\n",
    "\n",
    "# List by category\n",
    "print(\"\\nüéØ Classification models:\")\n",
    "for model in model_registry.list_models(category=\"classification\"):\n",
    "    print(f\"   ‚Ä¢ {model}\")\n",
    "\n",
    "print(\"\\nüìà Regression models:\")\n",
    "for model in model_registry.list_models(category=\"regression\"):\n",
    "    print(f\"   ‚Ä¢ {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data\n",
    "\n",
    "Let's create a binary classification dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    "    flip_y=0.1,\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_series = pd.Series(y, name=\"target\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y_series, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(f\"   Total samples: {len(X_df)}\")\n",
    "print(f\"   Features: {X_df.shape[1]}\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(\"\\n   Class distribution:\")\n",
    "print(y_series.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AutoML Pipeline - Model Comparison\n",
    "\n",
    "The AutoML pipeline automatically compares multiple models using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AutoML pipeline\n",
    "pipeline = AutoMLPipeline(task_type=\"classification\", random_state=42)\n",
    "\n",
    "# Compare models with 5-fold CV\n",
    "print(\"ü§ñ Comparing models... (this may take a minute)\\n\")\n",
    "results = pipeline.compare_models(X_train, y_train, cv=5, test_size=0.2)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüèÜ Model Comparison Results:\\n\")\n",
    "display(results)\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CV Mean scores\n",
    "results.plot(x=\"model\", y=\"cv_mean\", kind=\"barh\", ax=ax1, color=\"steelblue\")\n",
    "ax1.set_xlabel(\"Cross-Validation Mean Score\")\n",
    "ax1.set_title(\"Model Comparison - CV Scores\")\n",
    "\n",
    "# Test scores\n",
    "results.plot(x=\"model\", y=\"test_score\", kind=\"barh\", ax=ax2, color=\"coral\")\n",
    "ax2.set_xlabel(\"Test Score\")\n",
    "ax2.set_title(\"Model Comparison - Test Scores\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚ú® Best model: {pipeline.best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Let's evaluate the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = pipeline.get_best_model()\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred),\n",
    "    \"Recall\": recall_score(y_test, y_pred),\n",
    "    \"F1-Score\": f1_score(y_test, y_pred),\n",
    "}\n",
    "\n",
    "print(\"üìä Test Set Performance:\\n\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(f\"Confusion Matrix - {pipeline.best_model_name}\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance\n",
    "\n",
    "Understanding which features are most important for the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "if hasattr(best_model, \"get_feature_importance\"):\n",
    "    importance_df = best_model.get_feature_importance()\n",
    "\n",
    "    print(\"üîç Top 15 Most Important Features:\\n\")\n",
    "    display(importance_df.head(15))\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    importance_df.head(15).plot(\n",
    "        x=\"feature\", y=\"importance\", kind=\"barh\", color=\"green\", alpha=0.7\n",
    "    )\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Top 15 Feature Importances\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization\n",
    "\n",
    "Use Optuna to find optimal hyperparameters for the best model.\n",
    "\n",
    "**Note:** This cell may take several minutes to run depending on `n_trials`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run hyperparameter optimization\n",
    "# WARNING: This may take several minutes!\n",
    "\n",
    "# optimization_result = pipeline.optimize_hyperparameters(\n",
    "#     X_train, y_train,\n",
    "#     model_name=pipeline.best_model_name,\n",
    "#     n_trials=30,  # Increase for better results\n",
    "#     cv=5\n",
    "# )\n",
    "\n",
    "# print(\"‚öôÔ∏è Optimization Results:\\n\")\n",
    "# print(f\"   Best Score: {optimization_result['best_score']:.4f}\")\n",
    "# print(f\"\\n   Best Parameters:\")\n",
    "# for param, value in optimization_result['best_params'].items():\n",
    "#     print(f\"      {param}: {value}\")\n",
    "\n",
    "# # Get optimized model\n",
    "# optimized_model = optimization_result['model']\n",
    "# optimized_accuracy = optimized_model.score(X_test, y_test)\n",
    "# print(f\"\\n   Test Accuracy (optimized): {optimized_accuracy:.4f}\")\n",
    "\n",
    "print(\"üí° Uncomment the code above to run hyperparameter optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using Individual Models\n",
    "\n",
    "You can also use specific models directly with custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost classifier with custom parameters\n",
    "xgb_model = XGBoostClassifier(\n",
    "    n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "xgb_accuracy = xgb_model.score(X_test, y_test)\n",
    "print(f\"üéØ XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "\n",
    "# Get and display parameters\n",
    "params = xgb_model.get_params()\n",
    "print(\"\\nüìã Model Parameters:\")\n",
    "for key, value in list(params.items())[:5]:  # Show first 5\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(\"   ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sklearn Compatibility\n",
    "\n",
    "All ML engine models are fully sklearn-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create sklearn pipeline\n",
    "sklearn_pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"classifier\", XGBoostClassifier(n_estimators=50))]\n",
    ")\n",
    "\n",
    "# Use with cross_val_score\n",
    "cv_scores = cross_val_score(\n",
    "    sklearn_pipeline, X_train, y_train, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sklearn Compatibility Demo:\\n\")\n",
    "print(f\"   Cross-validation scores: {cv_scores}\")\n",
    "print(f\"   Mean CV score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "‚úÖ Model registry and discovery  \n",
    "‚úÖ Automated model comparison with AutoML  \n",
    "‚úÖ Model evaluation and metrics  \n",
    "‚úÖ Feature importance analysis  \n",
    "‚úÖ Hyperparameter optimization (optional)  \n",
    "‚úÖ Individual model usage  \n",
    "‚úÖ Sklearn compatibility  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. üìö Read the comprehensive [ML Engine Guide](../docs/ML_ENGINE_GUIDE.md)\n",
    "2. üî¨ Try with your own datasets\n",
    "3. ‚öôÔ∏è Experiment with hyperparameter optimization\n",
    "4. üéØ Integrate with MLflow for experiment tracking\n",
    "5. üöÄ Deploy your best model\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [README.md](../README.md) - Platform overview\n",
    "- [CLAUDE.md](../CLAUDE.md) - Development commands\n",
    "- [ML_ENGINE_GUIDE.md](../docs/ML_ENGINE_GUIDE.md) - Comprehensive guide\n",
    "- [ROADMAP.md](../ROADMAP.md) - Development roadmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
